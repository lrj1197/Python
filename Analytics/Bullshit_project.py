    '''
This Program allows one to input data in the form of
'source of bs # date gathered # type of bs(political, eduacational, social) # kind of bs(plots, tweet, etc)'
Then sift through the input and add the data to their lists. when finished enter 'end' to terminate the
program and then put the data into a pd.dataframe and pickle the file for later use.
'''
import matplotlib.pyplot as plt
#import the input algorithm
import sorting_alg
#import search_alg
import seaborn as sb
#gathter the inputs and add them into a list
#before each entry, clear the lists before running bullshit!!!!! nope

#call the bs func input algorithm
bullshit()

#check the lists to make sure things we input properly
#bs.clear()
#date.clear()
#typ.clear()
#kind.clear()
print(bs)
print(kind)
print(date)
print(typ)

#create the bs dataframe
Bullshit_dataset = pd.DataFrame({'Source of Bullshit' : bs, 'Date': date , 'type': typ, 'kind': kind})

#check the dataframe
Bullshit_dataset

#pickle the dataframe.
Bullshit_dataset.to_pickle('Bullshit_dataset_final.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_1.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_2.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_3.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_4.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_5.pkl')
#Bullshit_dataset.to_pickle('Bullshit_dataset_6.pkl')

#unpickle all the files
day0 = pd.read_pickle('Bullshit_dataset_2.pkl')
#day1 = pd.read_pickle('Bullshit_dataset_1.pkl')
#day2 = pd.read_pickle('Bullshit_dataset_2.pkl')
#day3 = pd.read_pickle('Bullshit_dataset_3.pkl')
#day4 = pd.read_pickle('Bullshit_dataset_4.pkl')
#day5 = pd.read_pickle('Bullshit_dataset_5.pkl')
#day6 = pd.read_pickle('Bullshit_dataset_6.pkl')
day0
#store all files in list
#files = [day0, day1, day2, day3, day4, day5, day6]

#merge all the dataframes into a master file
Bullshit_dataset_master = pd.concat([day0])

#repickle the master dataframe and create a master pickle
Bullshit_dataset_master.to_pickle('Bullshit_dataset_master.pkl')

'''
Now time for the analysis. Looking at the raw data, checking for negative times,
looking at grouping terms that are similar, ie (twitter, tweet, tweeted ... etc)
sort the data  and group the sources, report which place spread the most bs
and of what form. Generate plots and models to help explain... NO BIAS
'''

bsm = pd.read_pickle('Bullshit_dataset_final.pkl')

#look at the raw dataset
bsm

#sort through the dates and discard any erroneous values.
#check for any outliers.

#the sorting_alg takes care of most of the cleaning of the data. This is more of
#safty net in case the sorting_alg missed something.
bsm['Day'] = bsm['Date'].dt.day
#generate hist of the dates
plt.figure(); bsm['Day'].hist(bins = 7,label = 'Bullshit for the Week'); plt.show()

'''
1) What day was i given the most bs?

2) What type of bs was given the most? And from what source?

3) look at where the bs came from. How much was from news outlets? from me? from books, class, and school? the internet?
'''

bsm['Source of Bullshit']

me = ['luke', 'lucas', 'me', 'Luke', 'Lucas', 'Me']

#finds the values that are from me and returns the percentage of the total that came from me.
bsm_me = bsm['Source of Bullshit'][np.isin(bsm['Source of Bullshit'], me) == True]
bsm_me.dropna()

#get the length of the bsm_me dataframe and the master and calc/show the percentage of bs that comes
#from me.
m = len(bsm_me); b = len(bsm); x = m/b*100
print('The amount of BS the comes from me is, %f percent' % x)

#finds the values that are not from me
bsm_nme = bsm['Source of Bullshit'][np.isin(bsm['Source of Bullshit'], me) == False]
bsm_nme.dropna()

bsm_profs = bsm['Source of Bullshit'][np.isin(bsm['Source of Bullshit'], 'germs prof') == True]
prof = len(bsm_profs)

#get the length of the bsm dataframe and the master and calc/show the percentage of bs that comes
#from my professors and other sources.
j = prof/b*100
k = 100 - (j+x)
print('The amount of BS the comes from my professors is, %f percent' % j)
print('The amount of BS the Comes from other sources is, %f percent' % k)
'''
As of 3/8/18 I only contribute 29.4 %  of the BS in my life, granted that is mostly on purpose as an instructional tool for teaching
32.4 %  comes from my professors. The BS is mostly in the form of statistics and plots generated by third parties. My professorsare not
generating BS, they are mearly distrubuting it. 38 %  comes from other sources. This is mostly News outlets and news articles. Most of their
BS comes in the form of misquoting tweets or writing an article based on a tweet about a policy that was spoken about... a lot of places where
the full story could get lost.
'''

'''
Other questions to ask.

1) How much came from professors? from the News? Other sources?

2) Pie charts of the different datasets. Check

3) Look at the different types that came in and how many of the different types there are.

4)
'''

#gernerate a better lsit, or seperate the bsm[kind] to find if tweet or quote is in the name.
kd = ['quotes', 'tweets', 'tweet', 'Qoutes', 'Quote']

bsm['kind'][np.isin(bsm['kind'], kd) == True]

bsm['kind']


#faster way?
edu = bsm.type[np.isin(bsm.type, 'educational')]
e = len(edu)

polt = bsm.type[np.isin(bsm.type, 'political')]
pol = len(polt)

per = bsm.type[np.isin(bsm.type, 'personal')]
pe = len(per)

soc = bsm.type[np.isin(bsm.type, 'social')]
s = len(soc)

non = bsm.type[np.isin(bsm.type, '')]
n = len(non)

fracs = [pol,s,e,pe,n]

lables = ['political', 'social', 'educational', 'personal', 'None']


plt.figure();plt.pie(fracs, labels = lables, autopct='%.2f');plt.show()



bsm['edu'] = edu

f = []
for i in range(len(bsm)):
    if bsm['me'][i] == np.nan:
         if bsm['edu'][i] == np.nan:
                np.nan
    else:
        f.append(i)


bsm['combo'] = pd.DataFrame({'combo': f})


bsm

#bsm['me'] = bsm_me
#plt.figure(); bsm['mw'].hist(bins = 7,label = 'Bullshit for the Week'); plt.show()
